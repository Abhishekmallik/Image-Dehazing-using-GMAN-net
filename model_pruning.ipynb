{"cells":[{"metadata":{},"cell_type":"markdown","source":"Documentation and explanation available [here](tinyurl.com/gman-dehaze-net)  \n[Github Repository](https://github.com/sanchitvj/Image-Dehazing-using-GMAN-net)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q tensorflow_model_optimization","execution_count":1,"outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.3.3 is available.\r\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport glob\nimport random\nfrom PIL import Image\nimport time\nimport datetime\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import mean_squared_error\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow_model_optimization.sparsity import keras as sparsity\nimport tensorflow_model_optimization as tfmot","execution_count":28,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing and loading of data"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to load the image in the form of tensors.\n\ndef load_image(img_path):\n    img = tf.io.read_file(img_path)\n    img = tf.io.decode_jpeg(img, channels = 3)\n    img = tf.image.resize(img, size = (412, 548), antialias = True)\n    img = img / 255.0\n    return img","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to get the path of individual image.\n\ndef data_path(orig_img_path, hazy_img_path):\n    \n    train_img = []\n    val_img = []\n    \n    orig_img = glob.glob(orig_img_path + '/*.jpg')\n    n = len(orig_img)\n    random.shuffle(orig_img)\n    train_keys = orig_img[:int(0.9*n)]        #90% data for train, 10% for test\n    val_keys = orig_img[int(0.9*n):]\n    \n    split_dict = {}\n    for key in train_keys:\n        split_dict[key] = 'train'\n    for key in val_keys:\n        split_dict[key] = 'val'\n        \n    hazy_img = glob.glob(hazy_img_path + '/*.jpg')\n    for img in hazy_img:\n        img_name = img.split('/')[-1]\n        orig_path = orig_img_path + '/' + img_name.split('_')[0] + '.jpg'\n        if (split_dict[orig_path] == 'train'):\n            train_img.append([img, orig_path])\n        else:\n            val_img.append([img, orig_path])\n            \n    return train_img, val_img","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to load tensor image data in batches.\n\ndef dataloader(train_data, val_data, batch_size):\n    \n    train_data_orig = tf.data.Dataset.from_tensor_slices([img[1] for img in train_data]).map(lambda x: load_image(x))\n    train_data_haze = tf.data.Dataset.from_tensor_slices([img[0] for img in train_data]).map(lambda x: load_image(x))\n    train = tf.data.Dataset.zip((train_data_haze, train_data_orig)).shuffle(buffer_size=100).batch(batch_size)\n    \n    val_data_orig = tf.data.Dataset.from_tensor_slices([img[1] for img in val_data]).map(lambda x: load_image(x))\n    val_data_haze = tf.data.Dataset.from_tensor_slices([img[0] for img in val_data]).map(lambda x: load_image(x))\n    val = tf.data.Dataset.zip((val_data_haze, val_data_orig)).shuffle(buffer_size=100).batch(batch_size)\n    \n    return train, val","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# function to display output.\n\ndef display_img(model, hazy_img, orig_img):\n    \n    dehazed_img = model(hazy_img, training = True)\n    plt.figure(figsize = (15,12))\n    \n    display_list = [hazy_img[0], orig_img[0], dehazed_img[0]]\n    title = ['Hazy Image', 'Ground Truth', 'Dehazed Image']\n    \n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.title(title[i])\n        plt.imshow(display_list[i])\n        plt.axis('off')\n        \n    plt.show()","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Network Function"},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"def gman_net():\n    \n    inputs = tf.keras.Input(shape = [412, 548, 3])     # height, width of input image changed because of error in output\n    \n                                    ######################## GMAN Network ###########################\n        \n    conv = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                  bias_initializer = b_init, kernel_regularizer = regularizer)(inputs)\n    conv = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                  bias_initializer = b_init, kernel_regularizer = regularizer)(conv)\n    \n    \n                                    #### Encoding Layers #####\n    conv_up = Conv2D(filters = 128, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv)\n    conv_up = Conv2D(filters = 128, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv_up)\n                                    \n                                    #### Residual Layers #####\n    conv1_1 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                   bias_initializer = b_init, kernel_regularizer = regularizer)(conv_up)\n    conv1_2 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv1_1)\n    conv1_3 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                   bias_initializer = b_init, kernel_regularizer = regularizer)(conv1_2)\n    conc1 = tf.add(conv1_3, conv1_1)\n    conv1 = tf.keras.activations.relu(conc1)\n\n    conv2_1 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv1)\n    conv2_2 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv2_1)\n    conv2_3 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv2_2)\n    conc2 = tf.add(conv2_3, conv2_1)\n    conv2 = tf.keras.activations.relu(conc2)\n\n    conv3_1 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv2)\n    conv3_2 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv3_1)\n    conv3_3 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv3_2)\n    conv3_4 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv3_3)\n    conv3_5 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv3_4)\n    conc3 = tf.add(conv3_5, conv3_1)\n    conv3 = tf.keras.activations.relu(conc3)\n\n    conv4_1 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv3)\n    conv4_2 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv4_1)\n    conv4_3 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv4_2)\n    conv4_4 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv4_3)\n    conv4_5 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv4_4)\n    conc4 = tf.add(conv4_5, conv4_1)\n    conv4 = tf.keras.activations.relu(conc4)\n\n                                            ##### Decoding Layers #####\n    deconv = Conv2DTranspose(filters = 64, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                             kernel_regularizer = regularizer)(conv4)\n    deconv = Conv2DTranspose(filters = 64, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                             kernel_regularizer = regularizer)(deconv)\n\n    conv = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                  bias_initializer = b_init, kernel_regularizer = regularizer)(deconv)\n    conv = Conv2D(filters = 3, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                  bias_initializer = b_init, kernel_regularizer = regularizer)(conv)\n    conc = tf.add(conv, inputs)\n    gman_output = tf.keras.activations.relu(conc)\n    \n                               ######################## Parallel Network ###########################\n    \n    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 4, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                 kernel_regularizer = regularizer)(inputs)\n    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 2, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                 kernel_regularizer = regularizer)(conv)\n    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 2, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                 kernel_regularizer = regularizer)(conv)\n    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                 kernel_regularizer = regularizer)(conv)\n    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                 kernel_regularizer = regularizer)(conv)\n    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                 kernel_regularizer = regularizer)(conv)\n    deconv = Conv2DTranspose(filters = 64, kernel_size = 3, dilation_rate = 4, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                           activation = 'relu', kernel_regularizer = regularizer)(conv)\n    conv = Conv2D(filters = 3, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                 kernel_regularizer = regularizer)(deconv)\n    conc = tf.add(conv, inputs)\n    pn_output = tf.keras.activations.relu(conc)\n    \n    output = tf.add(gman_output, pn_output)\n    \n    return Model(inputs = inputs, outputs = output)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# Hyperparameters and config\nepochs = 1\nbatch_size = 8\nk_init = tf.keras.initializers.random_normal(stddev=0.008, seed = 101)      \nregularizer = tf.keras.regularizers.L2(1e-4)\nb_init = tf.constant_initializer()\n\ntrain_data, val_data = data_path(orig_img_path = '../input/dehaze/clear_images', hazy_img_path = '../input/dehaze/haze')\ntrain, val = dataloader(train_data, val_data, batch_size)\n\noptimizer = Adam(learning_rate = 1e-4)\n\n# for custom training loop\ntrain_loss_tracker = tf.keras.metrics.MeanSquaredError(name = \"train loss\")\nval_loss_tracker = tf.keras.metrics.MeanSquaredError(name = \"val loss\")","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This model was originally trained using custom loop below in the notebook. For pruning purpose using .fit method to\n# train 1 epoch.\n\ngman_net = gman_net()\ngman_net.compile(optimizer='adam', loss=tf.keras.losses.mean_squared_error, metrics=['mse'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngman_net.fit(train, epochs = 1, validation_data=val)","execution_count":20,"outputs":[{"output_type":"stream","text":"1462/1462 [==============================] - 1674s 1s/step - loss: 0.0147 - mse: 0.0114 - val_loss: 0.0083 - val_mse: 0.0076\nCPU times: user 20min 27s, sys: 28.6 s, total: 20min 55s\nWall time: 27min 59s\n","name":"stdout"},{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f2f707f9450>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"gman_net.save_weights('baseline.h5')   # saving weights\ngman_net.save('baseline')    # saving model","execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pruning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pruning network by training it from scratch, takes little more time than unpruned network.\n\nend_step = np.ceil(1.0 * 756 / 8).astype(np.int32) * epochs\n# print(end_step)\n\n# Pruning rate grows rapidly in the beginning from initial_sparsity, but then\n#     plateaus slowly to the target sparsity. The function applied is\n#     current_sparsity = final_sparsity + (initial_sparsity - final_sparsity)\n#           * (1 - (step - begin_step)/(end_step - begin_step)) ^ exponent\n# More details here: https://github.com/tensorflow/model-optimization/blob/c2642e5de64bb7709310bd7775de84b4765b359a/tensorflow_model_optimization/python/core/sparsity/keras/pruning_schedule.py#L183\nnew_pruning_params = {\n      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,final_sparsity=0.90,begin_step=0,end_step=end_step,\n                                                   frequency=100)}\n\n# prune_low_magnitude() wrapes the tensorflow model with pruning functionality\nnew_pruned_model = sparsity.prune_low_magnitude(gman_net(), **new_pruning_params)\n# new_pruned_model.summary()\nnet = new_pruned_model\n\nnet.compile(optimizer='adam', loss=tf.keras.losses.mean_squared_error, metrics=['mse'])","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nnet.fit(train, epochs = 1, validation_data=val, callbacks=[tensorflow_model_optimization.sparsity.keras.UpdatePruningStep()])","execution_count":16,"outputs":[{"output_type":"stream","text":"1431/1431 [==============================] - 1776s 1s/step - loss: 0.0195 - mse: 0.0148 - val_loss: 0.0127 - val_mse: 0.0120\nCPU times: user 21min 15s, sys: 32.6 s, total: 21min 47s\nWall time: 30min\n","name":"stdout"},{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f2f767b9b50>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"net.save_weights('pruned.h5')\nnet.save('pruned')","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pruning network from pre-trained network.\n\nend_step = np.ceil(1.0 * 756 / 8).astype(np.int32) * epochs\n# print(end_step)\n\nloaded_net = tf.keras.models.load_model('baseline')\npruning_saved_params = {\n      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,final_sparsity=0.90,begin_step=0,end_step=end_step,\n                                                   frequency=100)}\n\npruning_saved_model = sparsity.prune_low_magnitude(loaded_net, **pruning_saved_params)\n# new_pruned_model.summary()\n\n\npruning_saved_model.compile(optimizer='adam', loss=tf.keras.losses.mean_squared_error, metrics=['mse'])","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npruning_saved_model.fit(train, epochs = 1, validation_data=val, callbacks=[tensorflow_model_optimization.sparsity.keras.UpdatePruningStep()])","execution_count":37,"outputs":[{"output_type":"stream","text":"1462/1462 [==============================] - 1698s 1s/step - loss: 0.0083 - mse: 0.0081 - val_loss: 0.0078 - val_mse: 0.0076\nCPU times: user 21min 28s, sys: 28.7 s, total: 21min 57s\nWall time: 28min 34s\n","name":"stdout"},{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f2f6a926110>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pruning_saved_model.save('pruned_saved_model')\npruning_saved_model.save_weights('pruned_saved_model.h5')","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stripping the pruned network"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing pruning wrappers added to network.\n\nstrip_prune = tfmot.sparsity.keras.strip_pruning(net)\nstrip_prune_saved = tfmot.sparsity.keras.strip_pruning(pruning_saved_model)\n# stripe_prune.summary()","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strip_prune.save('striped')\nstrip_prune.save_weights('striped.h5')\nstrip_prune_saved.save('strip_prune_saved')\nstrip_prune_saved.save_weights('stripe_prune_saved.h5')","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# code snippet available on docs\ndef get_gzipped_size(keras_file,zip_name):\n    with zipfile.ZipFile(zip_name, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n        f.write(keras_file)\n\n    return os.path.getsize(zip_name)","execution_count":42,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performance Evaluation by comparing sizes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Below code can be used to check the weights pruned\n# for i, w in enumerate(model.get_weights()):\n#     print(\n#         \"{} -- Total parameters:{}, Pruned %: {:.2f}%\".format(\n#             model.weights[i].name, w.size, np.sum(w == 0) / w.size * 100\n#         )\n#     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Size of gzipped baseline model: %.2f bytes\" % (get_gzipped_size('./baseline', 'baseline_model.zip')))\nprint(\"Size of gzipped pruned model without stripping: %.2f bytes\" % (get_gzipped_size('./pruned', 'pruned_model.zip')))\nprint(\"Size of gzipped pruned model with stripping: %.2f bytes\" % (get_gzipped_size('./striped', 'striped_pruned_model.zip')))\nprint(\"Size of gzipped pretrained pruned model before stripping: %.2f bytes\" % (get_gzipped_size('./pruned_saved_model','pruned_saved_model.zip')))\nprint(\"Size of gzipped pretrained pruned model after stripping: %.2f bytes\" %(get_gzipped_size('./strip_prune_saved', 'strip_prune_saved.zip')))","execution_count":43,"outputs":[{"output_type":"stream","text":"Size of gzipped baseline model: 116.00 bytes\nSize of gzipped pruned model without stripping: 112.00 bytes\nSize of gzipped pruned model with stripping: 114.00 bytes\nSize of gzipped pretrained pruned model before stripping: 136.00 bytes\nSize of gzipped pretrained pruned model after stripping: 134.00 bytes\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Weight size of gzipped baseline model: %.2f bytes\" % (get_gzipped_size('./baseline.h5', 'baseline_model_w.zip')))\nprint(\"Weight size of gzipped pruned model without stripping: %.2f bytes\" % (get_gzipped_size('./pruned.h5', 'pruned_model_w.zip')))\nprint(\"Weight size of gzipped pruned model with stripping: %.2f bytes\" % (get_gzipped_size('./striped.h5', 'striped_pruned_model_w.zip')))\nprint(\"Weight size of gzipped pretrained pruned model before stripping: %.2f bytes\" % (get_gzipped_size('./pruned_saved_model.h5','pruned_saved_model_w.zip')))\nprint(\"Weight size of gzipped pretrained pruned model after stripping: %.2f bytes\" %(get_gzipped_size('./stripe_prune_saved.h5', 'strip_prune_saved_w.zip')))","execution_count":45,"outputs":[{"output_type":"stream","text":"Weight size of gzipped baseline model: 4544410.00 bytes\nWeight size of gzipped pruned model without stripping: 3242118.00 bytes\nWeight size of gzipped pruned model with stripping: 2863906.00 bytes\nWeight size of gzipped pretrained pruned model before stripping: 3105987.00 bytes\nWeight size of gzipped pretrained pruned model after stripping: 2746442.00 bytes\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Original training function"},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true},"cell_type":"code","source":"def train_model(epochs, train, val, net, train_loss_tracker, val_loss_tracker, optimizer):\n    \n    for epoch in range(epochs):\n        \n        print(\"\\nStart of epoch %d\" % (epoch,), end=' ')\n        start_time_epoch = time.time()\n        start_time_step = time.time()\n        \n        # training loop\n        \n        for step, (train_batch_haze, train_batch_orig) in enumerate(train):\n\n            with tf.GradientTape() as tape:\n\n                train_logits = net(train_batch_haze, training = True)\n                loss = mean_squared_error(train_batch_orig, train_logits)\n\n            grads = tape.gradient(loss, net.trainable_weights)\n            optimizer.apply_gradients(zip(grads, net.trainable_weights))\n\n            train_loss_tracker.update_state(train_batch_orig, train_logits)\n            if step == 0:\n                print('[', end='')\n            if step % 64 == 0:\n                print('=', end='')\n        \n        print(']', end='')\n        print('  -  ', end='')\n        print('Training Loss: %.4f' % (train_loss_tracker.result()), end='')\n        \n        # validation loop\n        \n        for step, (val_batch_haze, val_batch_orig) in enumerate(val):\n            val_logits = net(val_batch_haze, training = False)\n            val_loss_tracker.update_state(val_batch_orig, val_logits)\n            \n            if step % 32 ==0:\n                display_img(net, val_batch_haze, val_batch_orig)\n        \n        print('  -  ', end='')\n        print('Validation Loss: %.4f' % (val_loss_tracker.result()), end='')\n        print('  -  ', end=' ')\n        print(\"Time taken: %.2fs\" % (time.time() - start_time_epoch))\n        \n        net.save('trained_model')           # save the model(variables, weights, etc)\n        train_loss_tracker.reset_states()\n        val_loss_tracker.reset_states()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"%%time\ntrain_model(epochs, train, val, net, train_loss_tracker, val_loss_tracker, optimizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def evaluate(net, test_img_path):\n    \n    test_img = glob.glob(test_img_path + '/*.jpg')\n    random.shuffle(test_img)\n    \n    for img in test_img:\n        \n        img = tf.io.read_file(img)\n        img = tf.io.decode_jpeg(img, channels = 3)\n        \n        if img.shape[1] > img.shape[0]:\n            img = tf.image.resize(img, size = (1080, 1920), antialias = True)\n        if img.shape[1] < img.shape[0]:\n            img = tf.image.resize(img, size = (1920, 1080), antialias = True)\n        \n        img = img / 255.0\n        img = tf.expand_dims(img, axis = 0)      # transform input image from 3D to 4D\n        \n        dehaze = net(img, training = False)\n        \n        plt.figure(figsize = (80, 80))\n        \n        display_list = [img[0], dehaze[0]]       # make the first dimension zero\n        title = ['Hazy Image', 'Dehazed Image']\n\n        for i in range(2):\n            plt.subplot(1, 2, i+1)\n            plt.title(title[i], fontsize = 65, y = 1.045)\n            plt.imshow(display_list[i])\n            plt.axis('off')\n        \n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":true},"cell_type":"code","source":"test_net = tf.keras.models.load_model('trained_model', compile = False)\nevaluate(test_net, '../input/hazy-test-images')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}
